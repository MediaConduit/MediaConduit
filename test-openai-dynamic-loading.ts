#!/usr/bin/env tsx

/**
 * Test the OpenAI Provider Dynamic Loading from GitHub
 * 
 * This test verifies:
 * 1. Provider loads from GitHub repository
 * 2. Provider integrates with MediaConduit registry
 * 3. Multi-modal capabilities (Text-to-Text, Text-to-Image, Text-to-Audio, Audio-to-Text)
 * 4. Model instantiation and generation capabilities
 */

import { getProviderRegistry } from './src/media/registry/ProviderRegistry';
import { MediaCapability } from './src/media/types/provider';

async function testOpenAIProviderLoading() {
  console.log('üß™ Testing OpenAI Provider Dynamic Loading from GitHub...\n');

  try {
    // Test 1: Load Provider from GitHub
    console.log('üìã Test 1: Dynamic Provider Loading from GitHub');
    console.log('üîÑ Loading provider from: https://github.com/MediaConduit/openai-provider');
    
    const providerRegistry = getProviderRegistry();
    const provider = await providerRegistry.getProvider('https://github.com/MediaConduit/openai-provider');
    
    console.log(`‚úÖ Provider loaded successfully!`);
    console.log(`   Provider Name: ${provider.name}`);
    console.log(`   Provider ID: ${provider.id}`);
    console.log(`   Provider Type: ${provider.type}`);
    console.log(`   Capabilities: ${provider.capabilities.join(', ')}`);
    console.log('');

    // Test 2: Provider Configuration
    console.log('üìã Test 2: Provider Configuration');
    const apiKey = process.env.OPENAI_API_KEY;
    
    if (!apiKey) {
      console.log('‚ö†Ô∏è  OPENAI_API_KEY not found in environment variables');
      console.log('   Set OPENAI_API_KEY to test API functionality');
    } else {
      console.log('‚úÖ OpenAI API key found in environment');
    }
    console.log('');

    // Test 3: Multi-Modal Model Discovery
    console.log('üìã Test 3: Multi-Modal Model Discovery');
    
    const textToTextModels = provider.getModelsForCapability(MediaCapability.TEXT_TO_TEXT);
    const textToImageModels = provider.getModelsForCapability(MediaCapability.TEXT_TO_IMAGE);
    const textToAudioModels = provider.getModelsForCapability(MediaCapability.TEXT_TO_AUDIO);
    const audioToTextModels = provider.getModelsForCapability(MediaCapability.AUDIO_TO_TEXT);
    
    console.log(`‚úÖ Text-to-Text models: ${textToTextModels.length}`);
    console.log(`‚úÖ Text-to-Image models: ${textToImageModels.length}`);
    console.log(`‚úÖ Text-to-Audio models: ${textToAudioModels.length}`);
    console.log(`‚úÖ Audio-to-Text models: ${audioToTextModels.length}`);
    console.log(`‚úÖ Total models available: ${provider.models.length}`);
    
    if (textToTextModels.length > 0) {
      console.log('ü§ñ Available GPT models:');
      textToTextModels.slice(0, 8).forEach((model, index) => {
        console.log(`   ${index + 1}. ${model.name} (${model.id})`);
      });
    }
    
    if (textToImageModels.length > 0) {
      console.log('üé® Available DALL-E models:');
      textToImageModels.forEach((model, index) => {
        console.log(`   ${index + 1}. ${model.name} (${model.id})`);
      });
    }
    
    if (textToAudioModels.length > 0) {
      console.log('üîä Available TTS models:');
      textToAudioModels.forEach((model, index) => {
        console.log(`   ${index + 1}. ${model.name} (${model.id})`);
      });
    }
    
    if (audioToTextModels.length > 0) {
      console.log('üéôÔ∏è Available Speech models:');
      audioToTextModels.forEach((model, index) => {
        console.log(`   ${index + 1}. ${model.name} (${model.id})`);
      });
    }
    console.log('');

    // Test 4: Model Instantiation
    console.log('üìã Test 4: Model Instantiation');
    if (textToTextModels.length > 0) {
      try {
        // Prefer GPT-4o, fallback to GPT-4, then GPT-3.5
        let modelId = textToTextModels.find(m => m.id.includes('gpt-4o'))?.id || 
                     textToTextModels.find(m => m.id.includes('gpt-4'))?.id ||
                     textToTextModels.find(m => m.id.includes('gpt-3.5'))?.id ||
                     textToTextModels[0].id;
        
        console.log(`üîÑ Attempting to instantiate text model: ${modelId}`);
        
        const model = await provider.getModel(modelId);
        console.log(`‚úÖ Model instantiated successfully!`);
        console.log(`   Model ID: ${model.getId()}`);
        console.log(`   Model Name: ${model.getName()}`);
      } catch (error: any) {
        console.log(`‚ö†Ô∏è  Model instantiation error: ${error.message}`);
      }
    } else {
      console.log('‚ö†Ô∏è  No text-to-text models available for instantiation test');
    }
    console.log('');

    // Test 5: Availability Check
    console.log('üìã Test 5: Availability Check');
    const isAvailable = await provider.isAvailable();
    console.log(`‚úÖ Provider availability: ${isAvailable}`);
    
    if (!isAvailable) {
      console.log('   ‚ÑπÔ∏è  Provider not available (expected without API key)');
    }
    console.log('');

    // Test 6: OpenAI Model Portfolio Analysis
    console.log('üìã Test 6: OpenAI Model Portfolio Analysis');
    const gpt4oModels = textToTextModels.filter(m => m.id.includes('gpt-4o'));
    const gpt4Models = textToTextModels.filter(m => m.id.includes('gpt-4') && !m.id.includes('gpt-4o'));
    const gpt35Models = textToTextModels.filter(m => m.id.includes('gpt-3.5'));
    const dalleModels = textToImageModels.filter(m => m.id.includes('dall-e'));
    const ttsModels = textToAudioModels.filter(m => m.id.includes('tts'));
    const whisperModels = audioToTextModels.filter(m => m.id.includes('whisper'));
    
    console.log(`‚úÖ GPT-4o models available: ${gpt4oModels.length}`);
    console.log(`‚úÖ GPT-4 models available: ${gpt4Models.length}`);
    console.log(`‚úÖ GPT-3.5 models available: ${gpt35Models.length}`);
    console.log(`‚úÖ DALL-E models available: ${dalleModels.length}`);
    console.log(`‚úÖ TTS models available: ${ttsModels.length}`);
    console.log(`‚úÖ Whisper models available: ${whisperModels.length}`);
    console.log(`‚úÖ All models ready instantly (sync constructor pattern)`);
    console.log('');

    // Test 7: OpenAI Provider Ecosystem Assessment
    console.log('üìã Test 7: OpenAI Provider Ecosystem Assessment');
    console.log('ü§ñ OpenAI is the gold standard for AI because:');
    console.log('   - Most advanced GPT models (GPT-4o, GPT-4 Turbo)');
    console.log('   - Multimodal capabilities (text, image, audio)');
    console.log('   - Industry-leading performance and reliability');
    console.log('   - Comprehensive API with consistent interface');
    console.log('   - Cutting-edge models (DALL-E 3, Whisper, TTS)');
    console.log('   - Enterprise-grade security and compliance');
    console.log('   - Instant model availability (no async delays!)');
    console.log('');

    console.log('üéâ Dynamic OpenAI Provider Loading: SUCCESS!');
    console.log('');
    console.log('üìä Test Summary:');
    console.log('   ‚úÖ GitHub repository loading: PASSED');
    console.log('   ‚úÖ Provider instantiation: PASSED');
    console.log('   ‚úÖ Multi-modal discovery: PASSED');
    console.log('   ‚úÖ Model instantiation: TESTED');
    console.log('   ‚úÖ Model portfolio analysis: PASSED');
    console.log('   ‚úÖ AI ecosystem role: CONFIRMED');
    console.log('');
    console.log('üöÄ OpenAI Provider successfully migrated to dynamic system!');
    console.log('ü§ñ Ready to serve premium AI models across all modalities');

  } catch (error: any) {
    console.error('‚ùå Dynamic provider loading failed:', error.message);
    console.error('Full error:', error.stack);
    console.log('');
    console.log('üí° Troubleshooting tips:');
    console.log('   - Ensure Verdaccio is running on http://localhost:4873');
    console.log('   - Check that MediaConduit package is published to Verdaccio');
    console.log('   - Verify GitHub repository is accessible: https://github.com/MediaConduit/openai-provider');
    console.log('   - Check network connectivity');
    process.exit(1);
  }
}

// Run the test
testOpenAIProviderLoading().catch(console.error);
